import asyncio
import copy
import os.path
import random
import shutil
import subprocess
import time
from copy import deepcopy
from traceback import print_exc

import cv2
import streamlit as st
from streamlit_modal import Modal

from .chat_ai import *
from .chat_ai_openai import OpenaiResponse
from .config import *
from .utils import *
from .voice import _merge, get_all_voice, voice_main_return_async, voice_main_write, on_complete
from .yolo_infer import InferYOLO, resize_image, predict_image_use_resize, buffer_decode


def hidden():
    os.environ["PATH"] += os.pathsep + get_base_dir() + "/data/ffmpeg"  # è®©pydubå¯ä»¥æ‰¾åˆ°ffmpeg
    hide_st_style = """
                <style>
                #MainMenu {visibility: hidden;}
                footer {visibility: hidden;}
                header {visibility: hidden;}
                <![]()yle>
                """
    st.markdown(hide_st_style, unsafe_allow_html=True)

def _disable_chat_input():
    st.session_state.disable_chat_input = True

def create_location(location_dict):
    if "location" not in location_dict:
        st.session_state.location = ""
    with st.sidebar:
        st.markdown("è¯·åœ¨æ­¤è¾“å…¥èŒ¶å›­æ‰€åœ¨åœ°")
        col1, col2, col3 = st.columns(3)
        sheng = col1.selectbox(
            "çœä»½",
            key="sheng",
            options=["è¯·é€‰æ‹©"] + list(location_dict.keys())
        )
        # xian = col2.selectbox(
        #     "å¸‚åŒº",
        #     key="xian",
        #     options=[] if sheng == "è¯·é€‰æ‹©" else location_dict[sheng].keys(),
        # )
        # xiang = col3.selectbox(
        #     "ä¹¡å¿",
        #     key="xiang",
        #     options= [] if (xian is None or sheng == "è¯·é€‰æ‹©") else (location_dict[sheng][xian]),
        # )
        # if xian is not None and sheng != "è¯·é€‰æ‹©":
        #     if xiang is not None:
        #         st.session_state.location = sheng + xian + xiang
        #     else:
        #         st.session_state.location = sheng + xian
        xian = col2.selectbox(
            "å¸‚åŒº",
            key="xian",
            options=[] if sheng == "è¯·é€‰æ‹©" else ["è¯·é€‰æ‹©"] + list(location_dict[sheng].keys()),
        )
        xiang = col3.selectbox(
            "ä¹¡å¿",
            key="xiang",
            options= [] if (xian == "è¯·é€‰æ‹©" or sheng == "è¯·é€‰æ‹©") else (location_dict[sheng][xian]),
        )
        if xian != "è¯·é€‰æ‹©" and sheng != "è¯·é€‰æ‹©":
            if xiang is not None:
                st.session_state.location = sheng + xian + xiang
            else:
                st.session_state.location = sheng + xian
        st.markdown("---")


async def random_stream_text_async(ai, text, speed_range=(0.002, 0.06)):
    min_speed, max_speed = speed_range
    total = ""
    for char in text:
        total += char
        ai.write(total)  # æ¨¡æ‹Ÿæµå¼è¾“å‡º
        await asyncio.sleep(random.uniform(min_speed, max_speed))  # å¼‚æ­¥ç¡çœ 

def random_stream_text(ai, text, speed_range=(0.002, 0.06)):
    min_speed, max_speed = speed_range
    total = ""
    for char in text:
        total += char
        ai.write(total)  # æ¨¡æ‹Ÿæµå¼è¾“å‡º
        time.sleep(random.uniform(min_speed, max_speed))  # å¼‚æ­¥ç¡çœ 
        # è¿™ä¸ªæ˜¯çº¿ç¨‹é˜»å¡çš„

async def get_all_task(ai, text, speed_range=(0.002, 0.06), max_len=60):    # åˆ›å»ºä¸¤ä¸ªä»»åŠ¡å¹¶ç­‰å¾…å®ƒä»¬éƒ½å®Œæˆ
    task1 = asyncio.create_task(voice_main_return_async(text, max_len, on_complete))
    task2 = asyncio.create_task(random_stream_text_async(ai, text, speed_range))

    done, pending = await asyncio.wait([task1, task2], return_when=asyncio.FIRST_COMPLETED)
    if task2 in done and task1 not in done:
        st.info("è¯­éŸ³ç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™â€¦â€¦")
    results = await asyncio.gather(task1, task2)
    return results[0]

def create_select_model():
    st.markdown("""
    <style>
    .main div[data-testid="stSelectbox"] {
    position: fixed;   /* å›ºå®šå®šä½ */
    top: 4%;         /* è·é¡¶éƒ¨è·ç¦» */
    # left: 30%;
    z-index: 1000;     /* ç¡®ä¿åœ¨æœ€ä¸Šå±‚ */
    width: 200px !important;  /* è®¾ç½®å®½åº¦ */
    background: white;
    border-radius: 4px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}
/* è°ƒæ•´è¾“å…¥æ¡†éƒ¨åˆ† */
# div.stElementContainer.st-key-select_model
    .main div[data-testid="stSelectbox"] > div[role="combobox"] {
    padding: 4px 8px;  /* å†…è¾¹è· */
    min-height: 30px;  /* æœ€å°é«˜åº¦ */
}

/* è°ƒæ•´å­—ä½“å¤§å° */
   .main div[data-testid="stSelectbox"] {
    font-size: 1em !important;
}

/* è°ƒæ•´ä¸‹æ‹‰èœå•ä½ç½® */
   .main  div[data-testid="stSelectbox"] [role="listbox"] {
    transform: translateY(38px) !important;  /* ä¸‹æ‹‰èœå•ä½ç½®ä¿®æ­£ */
    width: 200px !important;  /* ä¸‹æ‹‰èœå•å®½åº¦ */
}
    /* è®©Modalè´´è¿‘é¡¶éƒ¨ */
    div[data-modal-container='true'][key='camera_modal'] > div:first-child > div:first-child{
        margin-top: -7% !important;
          overflow: visible !important; /* å–æ¶ˆæ»šåŠ¨ï¼Œå†…å®¹æº¢å‡ºæ—¶å…¨éƒ¨æ˜¾ç¤º */
      max-height: none !important;  /* å–æ¶ˆæœ€å¤§é«˜åº¦é™åˆ¶ */
      height: auto !important;      /* é«˜åº¦è‡ªé€‚åº”å†…å®¹ */
    }
    /* ä¼˜åŒ–ç§»åŠ¨ç«¯æ˜¾ç¤º */
    @media (max-width: 768px) {
        div[data-modal-container='true'][key='camera_modal'] > div:first-child > div:first-child{
            margin-top: -10% !important;
            padding: 9% !important;
              overflow: visible !important; /* å–æ¶ˆæ»šåŠ¨ï¼Œå†…å®¹æº¢å‡ºæ—¶å…¨éƒ¨æ˜¾ç¤º */
  max-height: none !important;  /* å–æ¶ˆæœ€å¤§é«˜åº¦é™åˆ¶ */
  height: auto !important;      /* é«˜åº¦è‡ªé€‚åº”å†…å®¹ */
        }
    }

    /* ç¾åŒ–ç›¸æœºè¾“å…¥ç»„ä»¶ */
    .stCamera > div {
        border-radius: 10px !important;
        overflow: hidden !important;
    }
    </style>
    """, unsafe_allow_html=True)

    select_model = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        ["è”ç½‘Agent", "å¾®è°ƒå¤šæ¨¡æ€"],
        key="select_model",
        label_visibility="collapsed"
    )
    if select_model != st.session_state.select_model:
        st.session_state.select_model = select_model
        st.rerun()

@st.cache_resource
def head_pic():
    return {"assistant": get_img_array("bot_head.png"),
            "user": get_img_array("user_head2.jpg")}

@st.cache_resource
def cache_model():
    class_names = ["back", 'èŒ¶ç…¤ç—…', 'èŒ¶é¥¼ç—…', 'èŒ¶è¤æ–‘ç—…', 'é—ä¼ æ€§ç—…å˜', 'èŒ¶ç°æ–‘ç—…', 'å¥åº·å¶ç‰‡', 'åœ°è¡£ç—…', 'ç¼ºé•ç—‡', 'èŒ¶è¨å®³',
                   'ç¼ºæ°®ç—‡', 'ç¼ºé’¾ç—‡', 'èŒ¶çº¢é”ˆç—…', 'ç¼ºç¡«ç—‡', 'æ—¥ç¼ç—…']  # ç›®å‰æ˜¯14ç§ç—…ç—‡
    model_dir = get_base_dir() + "/yolo_model/cloud_yoloe"  # Directory containing model.pdmodel and model.pdiparams
    infer = InferYOLO(model_dir=model_dir, class_names=class_names, use_gpu=False)
    return infer, class_names

@st.cache_resource
def cache_openai():
    return OpenaiResponse()

def create_camera():
    with st.session_state.modal.container():
        print_info("å·²æ‰“å¼€modal")
        st.session_state.camera_image = None
        st.markdown(
            "<h3 style='text-align: center; margin-top: 0;'>ğŸ“¸ ç‚¹å‡»ç›¸æœºä¸‹æ–¹çš„æŒ‰é’®ï¼ˆTake Photoï¼‰å³å¯æ‹ç…§</h3>"
            "<p style='text-align: right; margin-top: 0;'>ç‚¹å‡»è¿™ä¸‹é¢çš„ç›¸æœºæŒ‰é’®å¯åˆ‡æ¢å‰åæ‘„åƒå¤´</p>",
            unsafe_allow_html=True
        )
        print_info("å­˜åœ¨æ›´æ–°ç›¸æœºç»„ä»¶")
        st.session_state.camera_image = st.camera_input("ç›¸æœºå®æ—¶ç”»é¢")
        if st.session_state.camera_image is not None:
            st.session_state.uploader_key += 1  # è¿™é‡Œå°†ä¸Šä¼ å›¾ç‰‡çš„keyæ”¹å˜ï¼Œè¾¾åˆ°é‡ç½®ä¸Šä¼ å›¾ç‰‡çš„ç›®çš„
            st.session_state.need_yolo = True
            st.success("å·²æˆåŠŸæ‹ç…§")
            print_info("å·²æˆåŠŸæ‹ç…§")
            time.sleep(1)
            st.session_state.modal.close()
            st.rerun()

# @st.cache_resource
# def loading_video():
#     video = video_deal(get_base_dir() + "/data/loading.mp4")
#     print(type(video))
#     return video
# async def get_response_async(prompt):
#     if st.session_state.upload_file is not None:
#         fileres = FileResponse(st.session_state.base_response, "test.jpg")
#         task = asyncio.create_task(fileres.get_response_async(prompt, st.session_state.location))
#     else:
#         task = asyncio.create_task(st.session_state.base_response.get_response_async(prompt, st.session_state.location))
#
#     st.image()
#
#     await task

def _create_button(msgs: list):
    def _activate():
        st.session_state.messages = msgs
    st.button(msgs[0]["content"], use_container_width=True, on_click=_activate)

# def _judge_upload_file():
    # if isinstance(st.session_state.upload_file, UploadedFile):
    #     return True
    # else:
    #     return False

def get_response(prompt, model, use_star=True):
    """ä½¿ç”¨æ˜Ÿæ²³ç¤¾åŒºçš„æ¨¡å‹"""
    if model == "è”ç½‘Agent":
        # time.sleep(5)
        # return "testçš„æ— æ„ä¹‰å†…å®¹,è¯·å¿½ç•¥ï¼Œæˆ‘åªæ˜¯ç”¨æ¥å ä½çš„ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»ï¼Œæˆ‘é©¬ä¸Šå°±å¥½ï¼Œè°¢è°¢ï¼Œä½ å¥½æ£’ï¼Œç»§ç»­åŠ æ²¹ï¼æˆ‘ä¼šå˜å¾—æ›´å¥½ï¼ä½ ä¹Ÿä¼šçš„ï¼è¯¶å˜¿å˜¿ï¼Œä¹ˆä¹ˆå“’"
        retry = 0
        while retry < 5:
            try:
                now_img = _get_ont_img()
                if now_img is not None:
                    if not use_star:
                        if st.session_state.need_yolo:
                            print_info("éœ€è¦yolo")
                            fileres = FileResponse(st.session_state.base_response, file=now_img)
                        else:
                            fileres = FileResponse(st.session_state.base_response, "yolo_pic.jpg")
                        print_info("fileresæ­£å¸¸è¿è¡Œ")
                        response = fileres.get_response(prompt, st.session_state.location)
                    else:
                        if st.session_state.need_yolo:
                            print_info("éœ€è¦yolo")
                            response = cache_openai().cloud_process_big_small(prompt, st.session_state.location, file=now_img)
                        else:
                            response = cache_openai().cloud_process_big_small(prompt, st.session_state.location, file=st.session_state.yolo_pic, yolo_text=st.session_state.yolo_text)
                else:
                    if not use_star:
                        response = st.session_state.base_response.get_response(prompt, st.session_state.location)
                    else:
                        response = cache_openai().only_text(prompt, st.session_state.location)
                    # response = st.session_state.base_response.get_response(prompt, st.session_state.location)
                if use_star:
                    return response
                return response.json()["answer"]
            except Exception as e:
                print_exc()
                retry += 1
        else:
            return "ç–‘ä¼¼æœåŠ¡å™¨å‡ºç°é—®é¢˜ï¼Œéº»çƒ¦è”ç³»ç®¡ç†å‘˜"
    else:
        return "å½“å‰æ¨¡å‹è¿˜åœ¨å¾®è°ƒï¼Œæœªæ¥å°†ä¼šæ¥å…¥è¯¥ç½‘ç«™~è¯·å…ˆè°ƒæ•´å›è”ç½‘Agentæ¨¡å¼è¿›è¡Œä½¿ç”¨"

# def insert_video():
#     file_path = get_base_dir() + "/data/loading.mp4"
#
#     gif_html = f"""
#     <div style="display: flex; justify-content: center; align-items: center;">
#         <video autoplay loop muted playsinline style="pointer-events: none;">
#             <source src="{file_path}"type="video/mp4">
#             ä½ çš„è®¾å¤‡ä¸æ”¯æŒè¯¥åŠ è½½åŠ¨ç”»
#         </video>
#     </div>
#     """
#
#     # é€šè¿‡ markdown æ’å…¥ HTML
#     st.markdown(gif_html, unsafe_allow_html=True)
# def _save_upload_file(save_pic_name, resize=False):
#     assert st.session_state[f"upload_file_{st.session_state.uploader_key}"] is not None, "ä¸Šä¼ æ–‡ä»¶ä¸èƒ½ä¸ºç©ºæ‰å¯¹ï¼Œæœ‰é—®é¢˜"
#     image = Image.open(st.session_state[f"upload_file_{st.session_state.uploader_key}"])
#     rgb_image = image.convert("RGB")
#     save_pic_path = get_base_dir() + "/data/pic/" + save_pic_name
#     rgb_image.save(save_pic_path, format="JPEG")
#     if resize:
#         resize_path = resize_image(save_pic_path)
#     else:
#         resize_path = None
#     return save_pic_path, resize_path
def _save_upload_file(save_pic_name, resize=False):
    now_img = _get_ont_img()
    assert now_img is not None, "ä¸Šä¼ æ–‡ä»¶ä¸èƒ½ä¸ºç©ºæ‰å¯¹ï¼Œæœ‰é—®é¢˜"
    image = Image.open(now_img)
    rgb_image = image.convert("RGB")
    save_pic_path = get_base_dir() + "/data/pic/" + save_pic_name
    rgb_image.save(save_pic_path, format="JPEG")
    if resize:
        resize_path = resize_image(save_pic_path)
    else:
        resize_path = None
    return save_pic_path, resize_path

def _get_ont_img():
    if st.session_state[f"upload_file_{st.session_state.uploader_key}"] is not None or st.session_state.camera_image is not None:
        if st.session_state[f"upload_file_{st.session_state.uploader_key}"] is not None:
            print_info("ä½¿ç”¨ä¸Šä¼ å›¾ç‰‡")
            return st.session_state[f"upload_file_{st.session_state.uploader_key}"]
        else:
            print_info("ä½¿ç”¨ç›¸æœºæ‹ç…§çš„å›¾ç‰‡")
            return st.session_state.camera_image
    return None

def _reset_ss():
    st.session_state.uploader_key += 1
    st.session_state.need_yolo = True
    st.session_state.camera_image = None
    print_info("å·²æ¸…ç©º")
    st.rerun()

def _read_pic(pic_name):
    img_path = get_base_dir() + "/data/pic/" + pic_name
    img = cv2.imread(img_path)
    return deepcopy(img)

def clean_voice():
    print_info("æ­£åœ¨æ¸…ç©ºè¯­éŸ³æ–‡ä»¶å¤¹")
    if os.path.exists(get_base_dir() + "/data/voice"):
        shutil.rmtree(get_base_dir() + "/data/voice")
    if os.path.exists(get_base_dir() + "/data/voice/output_all.wav"):
        os.remove(get_base_dir() + "/data/voice/output_all.wav")
    os.makedirs(get_base_dir() + "/data/voice", exist_ok=True)

def main_chat_dialog():
    st.markdown(
        CSS,
        unsafe_allow_html=True
    )
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "base_response" not in st.session_state:
        st.session_state.base_response = AllResponse()
    if "disable_text_input" not in st.session_state:
        st.session_state.disable_text_input = True
    if "loading" not in st.session_state:
        st.session_state.loading = False
    if "history_conversations" not in st.session_state:
        st.session_state.history_conversations = []
    if "in_process" not in st.session_state:
        st.session_state.in_process = False
    if "prompt" not in st.session_state:
        st.session_state.prompt = ""
    if "need_yolo" not in st.session_state:
        st.session_state.need_yolo = True
    if "show_uploader" not in st.session_state:
        st.session_state.show_uploader = True
    if "uploader_key" not in st.session_state:
        st.session_state.uploader_key = 0
    if "camera_image" not in st.session_state:
        st.session_state.camera_image = None
    if "modal" not in st.session_state:
        st.session_state.modal = Modal(
        "ç›¸æœº",
        key="camera_modal",
        max_width=600
    )
    if "write" not in st.session_state:
        st.session_state.write = True
    if "yolo_pic" not in st.session_state:
        st.session_state.yolo_pic = None
    if "user_voice" not in st.session_state:
        st.session_state.user_voice = ""
    if "yolo_text" not in st.session_state:
        st.session_state.yolo_text = None

    with st.sidebar:

        uploader_container = st.empty()
        col1, col2 = uploader_container.columns([1, 1])
        if st.session_state.show_uploader:
            with col1:
                st.file_uploader(
                    "è¯·åœ¨è¿™é‡Œä¸Šä¼ èŒ¶å¶å›¾ç‰‡",
                    type=["png", "jpg", "jpeg"],
                    key=f"upload_file_{st.session_state.uploader_key}"
                )
                if st.session_state[f"upload_file_{st.session_state.uploader_key}"] is not None:
                    st.session_state.camera_image = None  # é‡ç½®æ‹ç…§ï¼ŒäºŒè€…åªèƒ½å­˜åœ¨ä¸€ä¸ª
            with col2:
                if st.button("ä½¿ç”¨ç›¸æœºæ‹ç…§", use_container_width=True):
                    st.session_state.modal.open()
                if st.button("æ¸…ç©ºå›¾ç‰‡", use_container_width=True):
                    _reset_ss()

                now_img = _get_ont_img()
                if now_img is not None:
                    st.image(now_img, caption="å½“å‰ç…§ç‰‡", use_column_width=True)
                else:
                    st.info("å½“å‰æ— å›¾ç‰‡")

        st.markdown("---")
        col1, col2 = st.columns([1, 1])

        if col1.button("å¼€å¯æ–°å¯¹è¯", use_container_width=True):
            if "messages" in st.session_state and len(st.session_state.messages) > 0:
                if st.session_state.messages not in st.session_state.history_conversations:
                    st.session_state.history_conversations.insert(0, st.session_state.messages)
                st.session_state.messages = []
            # é€šè¿‡æ”¹å˜ key æ¥é‡ç½®æ–‡ä»¶ä¸Šä¼ å™¨
            _reset_ss()

        if col2.button("ğŸ—‘ï¸æ¸…ç©ºå½“å‰å¯¹è¯", use_container_width=True):
            st.session_state.messages = []
            # é€šè¿‡æ”¹å˜ key æ¥é‡ç½®æ–‡ä»¶ä¸Šä¼ å™¨
            _reset_ss()
        if st.session_state.history_conversations:
            with st.expander("å†å²å¯¹è¯åˆ—è¡¨å¦‚ä¸‹"):
                for i in st.session_state.history_conversations:
                    _create_button(i)

    if st.session_state.modal.is_open():
        print_info("ç°åœ¨éœ€è¦ä½¿ç”¨ç›¸æœº")
        create_camera()


    for msg in st.session_state.messages:
        role = st.chat_message(msg["role"], avatar=head_pic()[msg["role"]])
        if "pic" in msg.keys():
            role.image(msg["pic"])
        role.write(msg["content"])
        if "audio" in msg.keys():
            role.audio(msg["audio"], format="audio/wav")

    st.session_state.disable_text_input = False if st.session_state.location != "" else True
    if st.session_state.location != "" :
        disable_text = "è¯·è¾“å…¥é—®é¢˜~è¿˜å¯ä»¥ä¸Šä¼ å›¾ç‰‡å“¦"
    else:
        disable_text = "è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½®ä½ç½®ï¼Œå†æ¥è¾“å…¥é—®é¢˜å¯ä»¥è·å¾—æ›´åŠ å‡†ç¡®çš„ç»“æœå“¦~"
    if st.session_state.in_process:
        disable_text = "ç°åœ¨æ­£åœ¨è¿è¡Œï¼Œè¯·ç­‰å¾…è¿è¡Œç»“æŸå†è¿›è¡Œè¾“å…¥~"
    with st.container():
        if st.session_state.write:
            col_11, col22, _ = st.columns([0.15, 0.99, 0.01])
            st.session_state.prompt = col22.chat_input(disable_text,
                                   disabled=st.session_state.disable_text_input or st.session_state.in_process,
                                   on_submit=_disable_chat_input,
                                   key="chat_input")
        else:
            col_11, col22, col33 = st.columns([0.15, 0.7, 0.3])
            with col22:
                # if st.button("æ‘ä¸‹å¼€å§‹å½•éŸ³",
                #                  help=disable_text,
                #                  disabled=st.session_state.disable_text_input or st.session_state.in_process,
                #                  use_container_width=True,
                #                  ): # TODOï¼š è¿™é‡Œå½•éŸ³åŠŸèƒ½æš‚æ—¶å…³é—­
                if st.button("ç”±äºæœåŠ¡å™¨é—®é¢˜ï¼Œå½•éŸ³åŠŸèƒ½æš‚æ—¶å…³é—­ï¼Œè¯·ä½¿ç”¨æ–‡å­—è¾“å…¥",
                             help=disable_text,
                             disabled=True,
                             use_container_width=True,
                             ):
                    st.session_state.user_voice = voice_main_write()
            with col33:
                if st.button("ä¸Šä¼ ",
                              disabled=st.session_state.disable_text_input or st.session_state.in_process or st.session_state.user_voice == "",
                              use_container_width=True,
                              on_click=_disable_chat_input()):
                    st.session_state.prompt = st.session_state.user_voice
                    st.session_state.user_voice = ""
        with col_11:
            mode = "åˆ‡æ¢è¯­éŸ³è¾“å…¥" if st.session_state.write else "åˆ‡æ¢æ–‡å­—è¾“å…¥"
            if st.button(mode, disabled=st.session_state.in_process, use_container_width=True):
                st.session_state.write = not st.session_state.write
                st.rerun()

    if st.session_state.prompt or st.session_state.in_process:

        if not st.session_state.in_process:
            clean_voice()  # TODO:è¿™é‡Œæ¸…ç©ºè¯­éŸ³æ–‡ä»¶å¤¹ï¼Œå¯ä»¥åˆ é™¤
            st.session_state.messages.append({"role": "user", "content": st.session_state.prompt})
            # user = st.chat_message("user", avatar=head_pic()["user"])
            # with user:
            #     if st.session_state.upload_file is not None:
            #         st.write("å·²ä¸Šä¼ :" + st.session_state.upload_file.name)
            #         st.image(st.session_state.upload_file)
            #
            #     st.write(prompt)
            now_img = _get_ont_img()
            if now_img is not None:
                st.session_state.messages[-1]["pic"] = copy.deepcopy(now_img)
            st.session_state.in_process = True
            st.rerun()
        st.session_state.prompt = st.session_state.messages[-1]["content"]  # è¿™é‡Œé‡æ–°èµ‹å€¼ï¼Œå› ä¸ºst.rerunä¼šå°†è¾“å…¥å†…å®¹å…¨éƒ¨æ¸…ç©ºï¼Œå¯¼è‡´rerunä¹‹åst.prompté‡æ–°èµ‹å€¼ï¼Œé‡æ–°èµ‹å€¼å°±å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Œè¿™é‡Œä¸ºNone
        assistant_message = st.chat_message("assistant", avatar=head_pic()["assistant"])
        now_img = _get_ont_img()
        if now_img is not None:
            # if st.session_state.need_yolo:
            infer = cache_model()[0]
            user_pic_path, resize_path = _save_upload_file("user_upload.jpg", True)
            # predict_image(infer, "user_upload.jpg", "yolo_pic.jpg", conf_threshold=0.45)
            st.session_state.yolo_pic, st.session_state.yolo_text = predict_image_use_resize(infer,
                                                                 user_pic_path,
                                                                 resize_path,
                                                                 "",    # æš‚æ—¶å¼ƒç”¨
                                                                 class_names=cache_model()[1],
                                                                 conf_threshold=0.7)
            st.session_state.need_yolo = False
            assistant_message.image(st.session_state.yolo_pic)# yolo_pic_path)
            # st.session_state.upload_file = upload_file_path  # assistant_message.image(st.session_state.upload_file)

        #     st.session_state.upload_file = None
        assistant_message_placeholder = assistant_message.empty()
        with assistant_message_placeholder.container():
            show1, show2 = st.columns([1, 8])
            # show1.video(get_base_dir() + "/data/loading.mp4", loop=True)
            with show1:
                # st.markdown('<div class="loading_gif">', unsafe_allow_html=True)
                show1.image(get_base_dir() + "/data/loading_gif.gif")
                # st.markdown('</div>', unsafe_allow_html=True)
            # with show1:
            #     insert_video()
            show2.markdown("<h2>å¤§æ¨¡å‹æ­£åœ¨æ€è€ƒâ€¦â€¦</h2>", unsafe_allow_html=True)
            print_info("ç­‰å¾…è¾“å‡ºä¸­â€¦â€¦è¾“å…¥ä¸º", st.session_state.prompt)
            full_reply = get_response(st.session_state.prompt, st.session_state.select_model)
        print_info(os.path.exists(get_base_dir() + "/data/voice/ori_text.txt"))
        # with open(get_base_dir() + "/data/voice/ori_text.txt", "w", encoding="utf-8") as f:
        #     f.write(full_reply)
        # print_info(os.path.exists(get_base_dir() + "/data/voice/ori_text.txt"))
        # print_info("å·²å†™å…¥æ–‡æœ¬ï¼Œå¼€å§‹ç”Ÿæˆè¯­éŸ³")
        # process = subprocess.Popen(
        #     f'python -m src.voice "ori_text.txt"',
        #     cwd=get_base_dir(),
        #     shell=True,
        # )

        # print_info("æ ‡å‡†è¾“å‡º:", result.stdout)
        # print_info("æ ‡å‡†é”™è¯¯:", result.stderr)
        # print_info("é€€å‡ºç :", result.returncode)
        assistant_message_placeholder.empty()  # æ¸…ç©ºå†…å®¹
        audio = asyncio.run(get_all_task(assistant_message_placeholder, full_reply))


        # random_stream_text(assistant_message_placeholder, full_reply)

        voice_assistant = assistant_message.empty()
        # length = len(split_str_length(full_reply))
        # while True:
        #     if os.path.exists(get_base_dir() + "/data/voice/output_all.wav"):
        #         voice_assistant.empty()  # æ¸…ç©ºå†…å®¹
        #         break
        #     else:
        #         voice_list = [i for i in os.listdir(get_base_dir() + "/data/voice") if i.endswith(".wav") and i != "output_all.wav"]
        #         if len(voice_list) == length:
        #             _merge()  # å¦‚æœå¤Ÿäº†ç›´æ¥åˆå¹¶éŸ³é¢‘
        #             break
        #         else:
        #             voice_assistant.empty()
        #             voice_assistant.info(f"æ­£åœ¨ç”Ÿæˆè¯­éŸ³...å·²å®Œæˆ{len(voice_list)}/{length}")
        #     time.sleep(5)

        voice_assistant.audio(deepcopy(audio), format="audio/wav")
        st.session_state.messages.append({"role": "assistant", "content": full_reply})
        if now_img is not None:
            st.session_state.messages[-1]["pic"] = st.session_state.yolo_pic
        st.session_state.messages[-1]["audio"] = deepcopy(audio)
        st.session_state.disable_text_input = False
        st.session_state.in_process = False
        st.session_state.prompt = ""
        st.rerun()
        # st.session_state.messages.append({"role": "assistant", "content": full_reply})

    if not st.session_state.messages:
        st.markdown("""
    # ğŸŒ± æ¬¢è¿ä½¿ç”¨ TeaVisor

    âœ¨ **æ™ºèƒ½åŠ©åŠ›èŒ¶å¶ç§æ¤**
    - **å¤šæ¨¡æ€æ™ºèƒ½åˆ†æ**ï¼šç»“åˆå›¾ç‰‡ä¸åŸºäºç§æ¤åœ°æ°”å€™ä¸å¤©æ°”é¢„æŠ¥ï¼Œæ·±å…¥è¯Šæ–­èŒ¶å¶ç”Ÿé•¿çŠ¶å†µä¸ç—…è™«å®³é—®é¢˜
    - **æ™ºèƒ½æœç´¢å¼•æ“**ï¼šç»“åˆç™¾åº¦æ™ºèƒ½æœç´¢ï¼Œè·å¾—ä¸æ—¶ä¿±è¿›çš„è§£å†³æ–¹æ¡ˆ
    - **ç®€å•æ˜“ç”¨**ï¼šæ‰‹æœºå³å¼€å³ç”¨ï¼Œæ— ä½¿ç”¨é—¨æ§›
    ---
    ğŸ“Œ **æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸€ç›®äº†ç„¶**ï¼
    - **é…ç½®ä½ç½®**ï¼š**è¯·å…ˆåœ¨ä¾§è¾¹æ è®¾ç½®ç§æ¤åœ°ä½ç½®**ï¼Œç³»ç»Ÿä¼šè·å–å½“åœ°æ°”å€™ä¿¡æ¯ã€‚
    - **ä¸Šä¼ å›¾ç‰‡**ï¼š**ä¸Šä¼ èŒ¶å¶å›¾ç‰‡**è¯†åˆ«ï¼Œè¯Šæ–­ç”Ÿé•¿çŠ¶å†µå’Œç—…è™«å®³ã€‚
    - **è¾“å…¥é—®é¢˜**ï¼šåœ¨**ä¸‹æ–¹è¾“å…¥æ è¾“å…¥**æ‚¨çš„é—®é¢˜ï¼Œä¹‹åæ¨¡å‹å°±ä¼šå›ç­”æ‚¨~
            """)